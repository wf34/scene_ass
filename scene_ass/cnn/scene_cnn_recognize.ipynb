{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitri/projects/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/dmitri/projects/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/dmitri/projects/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import caffe\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acquire loaded weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe_root = '/home/dmitri/projects/caffe/'\n",
    "pretrained_weights_path = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\n",
    "assert os.path.exists(pretrained_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define default structure of caffenet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "weight_param = dict(lr_mult=1, decay_mult=1)\n",
    "bias_param   = dict(lr_mult=2, decay_mult=0)\n",
    "learned_param = [weight_param, bias_param]\n",
    "\n",
    "frozen_param = [dict(lr_mult=0)] * 2\n",
    "\n",
    "def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1,\n",
    "              param=learned_param,\n",
    "              weight_filler=dict(type='gaussian', std=0.01),\n",
    "              bias_filler=dict(type='constant', value=0.1)):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                         num_output=nout, pad=pad, group=group,\n",
    "                         param=param, weight_filler=weight_filler,\n",
    "                         bias_filler=bias_filler)\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "def fc_relu(bottom, nout, param=learned_param,\n",
    "            weight_filler=dict(type='gaussian', std=0.005),\n",
    "            bias_filler=dict(type='constant', value=0.1)):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout, param=param,\n",
    "                        weight_filler=weight_filler,\n",
    "                        bias_filler=bias_filler)\n",
    "    return fc, L.ReLU(fc, in_place=True)\n",
    "\n",
    "def max_pool(bottom, ks, stride=1):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
    "\n",
    "def caffenet(data,\n",
    "             label=None,\n",
    "             train=True,\n",
    "             num_classes=1000,\n",
    "             classifier_name='fc8',\n",
    "             learn_all=False):\n",
    "    \"\"\"Returns a NetSpec specifying CaffeNet, following the original proto text\n",
    "       specification (./models/bvlc_reference_caffenet/train_val.prototxt).\"\"\"\n",
    "    n = caffe.NetSpec()\n",
    "    n.data = data\n",
    "    param = learned_param if learn_all else frozen_param\n",
    "    n.conv1, n.relu1 = conv_relu(n.data, 11, 96, stride=4, param=param)\n",
    "    n.pool1 = max_pool(n.relu1, 3, stride=2)\n",
    "    n.norm1 = L.LRN(n.pool1, local_size=5, alpha=1e-4, beta=0.75)\n",
    "    n.conv2, n.relu2 = conv_relu(n.norm1, 5, 256, pad=2, group=2, param=param)\n",
    "    n.pool2 = max_pool(n.relu2, 3, stride=2)\n",
    "    n.norm2 = L.LRN(n.pool2, local_size=5, alpha=1e-4, beta=0.75)\n",
    "    n.conv3, n.relu3 = conv_relu(n.norm2, 3, 384, pad=1, param=param)\n",
    "    n.conv4, n.relu4 = conv_relu(n.relu3, 3, 384, pad=1, group=2, param=param)\n",
    "    n.conv5, n.relu5 = conv_relu(n.relu4, 3, 256, pad=1, group=2, param=param)\n",
    "    n.pool5 = max_pool(n.relu5, 3, stride=2)\n",
    "    n.fc6, n.relu6 = fc_relu(n.pool5, 4096, param=param)\n",
    "    if train:\n",
    "        n.drop6 = fc7input = L.Dropout(n.relu6, in_place=True)\n",
    "    else:\n",
    "        fc7input = n.relu6\n",
    "        \n",
    "    n.fc7, n.relu7 = fc_relu(fc7input, 4096, param=param)\n",
    "    if train:\n",
    "        n.drop7 = fc8input = L.Dropout(n.relu7, in_place=True)\n",
    "    else:\n",
    "        fc8input = n.relu7\n",
    "    \n",
    "    # always learn fc8 (param=learned_param)\n",
    "    fc8 = L.InnerProduct(fc8input, num_output=num_classes, param=learned_param)\n",
    "    # give fc8 the name specified by argument `classifier_name`\n",
    "    n.__setattr__(classifier_name, fc8)\n",
    "    if not train:\n",
    "        n.probs = L.Softmax(fc8)\n",
    "    if label is not None:\n",
    "        n.label = label\n",
    "        n.loss = L.SoftmaxWithLoss(fc8, n.label)\n",
    "        n.acc = L.Accuracy(fc8, n.label)\n",
    "    # write the net to a temporary file and return its filename\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(n.to_proto()))\n",
    "        print('created net there ->', f.name)\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_style_recognize_net(train=True, learn_all=False, subset=None):\n",
    "    if subset is None:\n",
    "        subset = 'train' if train else 'test'\n",
    "    source = '/home/dmitri/projects/graphicon_paper/dataset/video10_train2/{}.txt'.format(subset)\n",
    "    assert os.path.isfile(source), source\n",
    "    mean_file_ = caffe_root + 'data/ilsvrc12/scene_ass_mean.binaryproto' #scene_ass imagenet\n",
    "    assert os.path.isfile(mean_file_), mean_file_\n",
    "    transform_param = dict(mirror=train,\n",
    "                           crop_size=227,\n",
    "                           mean_file=mean_file_)\n",
    "    \n",
    "    style_data, style_label = L.ImageData(transform_param = transform_param,\n",
    "                                          source = source,\n",
    "                                          batch_size = 50,\n",
    "                                          new_height = 256,\n",
    "                                          new_width = 256,\n",
    "                                          ntop = 2)\n",
    "    \n",
    "    return caffenet(data = style_data,\n",
    "                    label = style_label,\n",
    "                    train = train,\n",
    "                    num_classes = 2,\n",
    "                    classifier_name = 'fc8_sceneassoc',\n",
    "                    learn_all = learn_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "def solver(train_net_path, test_net_path=None, base_lr=0.001):\n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    if test_net_path is not None:\n",
    "        s.test_net.append(test_net_path)\n",
    "        s.test_interval = 1000  # Test after every 1000 training iterations.\n",
    "        s.test_iter.append(100) # Test on 100 batches each time we test.\n",
    "\n",
    "    # The number of iterations over which to average the gradient.\n",
    "    # Effectively boosts the training batch size by the given factor, without\n",
    "    # affecting memory utilization.\n",
    "    s.iter_size = 1\n",
    "    \n",
    "    s.max_iter = 100000     # # of times to update the net (training iterations)\n",
    "    \n",
    "    # Solve using the stochastic gradient descent (SGD) algorithm.\n",
    "    # Other choices include 'Adam' and 'RMSProp'.\n",
    "    s.type = 'SGD'\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    s.base_lr = base_lr\n",
    "\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # Here, we 'step' the learning rate by multiplying it by a factor `gamma`\n",
    "    # every `stepsize` iterations.\n",
    "    s.lr_policy = 'step'\n",
    "    s.gamma = 0.1\n",
    "    s.stepsize = 20000\n",
    "\n",
    "    # Set other SGD hyperparameters. Setting a non-zero `momentum` takes a\n",
    "    # weighted average of the current gradient and previous gradients to make\n",
    "    # learning more stable. L2 weight decay regularizes learning, to help prevent\n",
    "    # the model from overfitting.\n",
    "    s.momentum = 0.9\n",
    "    s.weight_decay = 5e-4\n",
    "\n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.  Here, we'll\n",
    "    # snapshot every 10K iterations -- ten times during training.\n",
    "    s.snapshot = 10000\n",
    "    s.snapshot_prefix = os.path.dirname(os.path.abspath('.'))\n",
    "    \n",
    "    # Train on the GPU.  Using the CPU to train large networks is very slow.\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    # Write the solver to a temporary file and return its filename.\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(s))\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_solvers(niter, solvers, disp_interval=10):\n",
    "    \"\"\"Run solvers for niter iterations,\n",
    "       returning the loss and accuracy recorded each iteration.\n",
    "       `solvers` is a list of (name, solver) tuples.\"\"\"\n",
    "    blobs = ('loss', 'acc')\n",
    "    loss, acc = ({name: np.zeros(niter) for name, _ in solvers}\n",
    "                 for _ in blobs)\n",
    "    for it in range(niter):\n",
    "        for name, s in solvers:\n",
    "            s.step(1)  # run a single SGD step in Caffe\n",
    "            loss[name][it], acc[name][it] = (s.net.blobs[b].data.copy()\n",
    "                                             for b in blobs)\n",
    "        if it % disp_interval == 0 or it + 1 == niter:\n",
    "            loss_disp = '; '.join('%s: loss=%.3f, acc=%2d%%' %\n",
    "                                  (n, loss[n][it], np.round(100*acc[n][it]))\n",
    "                                  for n, _ in solvers)\n",
    "            print '%3d) %s' % (it, loss_disp)     \n",
    "    # Save the learned weights from both nets.\n",
    "    weight_dir = tempfile.mkdtemp()\n",
    "    weights = {}\n",
    "    for name, s in solvers:\n",
    "        filename = 'weights.%s.caffemodel' % name\n",
    "        weights[name] = os.path.join(weight_dir, filename)\n",
    "        s.net.save(weights[name])\n",
    "    return loss, acc, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('created net there ->', '/tmp/tmpzYu7GG')\n",
      "/tmp/tmpdmkGoZ\n",
      "Running solver for 10000 iterations...\n",
      "  0) pretrained: loss=0.693, acc=30%\n",
      " 10) pretrained: loss=0.319, acc=80%\n",
      " 20) pretrained: loss=0.482, acc=78%\n",
      " 30) pretrained: loss=0.277, acc=90%\n",
      " 40) pretrained: loss=0.095, acc=100%\n",
      " 50) pretrained: loss=0.132, acc=96%\n",
      " 60) pretrained: loss=0.252, acc=92%\n",
      " 70) pretrained: loss=0.287, acc=92%\n",
      " 80) pretrained: loss=0.317, acc=90%\n",
      " 90) pretrained: loss=0.121, acc=98%\n",
      "100) pretrained: loss=0.228, acc=94%\n",
      "110) pretrained: loss=0.110, acc=94%\n",
      "120) pretrained: loss=0.106, acc=98%\n",
      "130) pretrained: loss=0.102, acc=96%\n",
      "140) pretrained: loss=0.284, acc=92%\n",
      "150) pretrained: loss=0.084, acc=96%\n",
      "160) pretrained: loss=0.148, acc=94%\n",
      "170) pretrained: loss=0.054, acc=98%\n",
      "180) pretrained: loss=0.128, acc=94%\n",
      "190) pretrained: loss=0.044, acc=100%\n",
      "200) pretrained: loss=0.060, acc=100%\n",
      "210) pretrained: loss=0.129, acc=94%\n",
      "220) pretrained: loss=0.116, acc=94%\n",
      "230) pretrained: loss=0.103, acc=96%\n",
      "240) pretrained: loss=0.062, acc=98%\n",
      "250) pretrained: loss=0.088, acc=96%\n",
      "260) pretrained: loss=0.113, acc=98%\n",
      "270) pretrained: loss=0.102, acc=96%\n",
      "280) pretrained: loss=0.022, acc=100%\n",
      "290) pretrained: loss=0.019, acc=100%\n",
      "300) pretrained: loss=0.045, acc=96%\n",
      "310) pretrained: loss=0.107, acc=98%\n",
      "320) pretrained: loss=0.093, acc=96%\n",
      "330) pretrained: loss=0.070, acc=96%\n",
      "340) pretrained: loss=0.037, acc=98%\n",
      "350) pretrained: loss=0.011, acc=100%\n",
      "360) pretrained: loss=0.224, acc=94%\n",
      "370) pretrained: loss=0.164, acc=92%\n",
      "380) pretrained: loss=0.008, acc=100%\n",
      "390) pretrained: loss=0.019, acc=100%\n",
      "400) pretrained: loss=0.045, acc=100%\n",
      "410) pretrained: loss=0.133, acc=96%\n",
      "420) pretrained: loss=0.070, acc=98%\n",
      "430) pretrained: loss=0.048, acc=96%\n",
      "440) pretrained: loss=0.109, acc=98%\n",
      "450) pretrained: loss=0.077, acc=98%\n",
      "460) pretrained: loss=0.159, acc=96%\n",
      "470) pretrained: loss=0.035, acc=98%\n",
      "480) pretrained: loss=0.080, acc=98%\n",
      "490) pretrained: loss=0.011, acc=100%\n",
      "500) pretrained: loss=0.096, acc=94%\n",
      "510) pretrained: loss=0.252, acc=92%\n",
      "520) pretrained: loss=0.068, acc=96%\n",
      "530) pretrained: loss=0.008, acc=100%\n",
      "540) pretrained: loss=0.044, acc=98%\n",
      "550) pretrained: loss=0.011, acc=100%\n",
      "560) pretrained: loss=0.188, acc=96%\n",
      "570) pretrained: loss=0.019, acc=98%\n",
      "580) pretrained: loss=0.114, acc=98%\n",
      "590) pretrained: loss=0.016, acc=100%\n",
      "600) pretrained: loss=0.066, acc=98%\n",
      "610) pretrained: loss=0.003, acc=100%\n",
      "620) pretrained: loss=0.034, acc=98%\n",
      "630) pretrained: loss=0.021, acc=100%\n",
      "640) pretrained: loss=0.027, acc=100%\n",
      "650) pretrained: loss=0.021, acc=100%\n",
      "660) pretrained: loss=0.036, acc=98%\n",
      "670) pretrained: loss=0.024, acc=100%\n",
      "680) pretrained: loss=0.034, acc=100%\n",
      "690) pretrained: loss=0.035, acc=100%\n",
      "700) pretrained: loss=0.015, acc=100%\n",
      "710) pretrained: loss=0.085, acc=96%\n",
      "720) pretrained: loss=0.050, acc=98%\n",
      "730) pretrained: loss=0.006, acc=100%\n",
      "740) pretrained: loss=0.009, acc=100%\n",
      "750) pretrained: loss=0.143, acc=98%\n",
      "760) pretrained: loss=0.058, acc=96%\n",
      "770) pretrained: loss=0.014, acc=100%\n",
      "780) pretrained: loss=0.129, acc=96%\n",
      "790) pretrained: loss=0.034, acc=98%\n",
      "800) pretrained: loss=0.121, acc=98%\n",
      "810) pretrained: loss=0.089, acc=98%\n",
      "820) pretrained: loss=0.033, acc=98%\n",
      "830) pretrained: loss=0.048, acc=98%\n",
      "840) pretrained: loss=0.028, acc=100%\n",
      "850) pretrained: loss=0.017, acc=100%\n",
      "860) pretrained: loss=0.027, acc=98%\n",
      "870) pretrained: loss=0.072, acc=96%\n",
      "880) pretrained: loss=0.199, acc=98%\n",
      "890) pretrained: loss=0.033, acc=98%\n",
      "900) pretrained: loss=0.039, acc=98%\n",
      "910) pretrained: loss=0.031, acc=98%\n",
      "920) pretrained: loss=0.058, acc=98%\n",
      "930) pretrained: loss=0.023, acc=98%\n",
      "940) pretrained: loss=0.045, acc=98%\n",
      "950) pretrained: loss=0.007, acc=100%\n",
      "960) pretrained: loss=0.046, acc=98%\n",
      "970) pretrained: loss=0.017, acc=100%\n",
      "980) pretrained: loss=0.058, acc=98%\n",
      "990) pretrained: loss=0.006, acc=100%\n",
      "1000) pretrained: loss=0.009, acc=100%\n",
      "1010) pretrained: loss=0.010, acc=100%\n",
      "1020) pretrained: loss=0.020, acc=98%\n",
      "1030) pretrained: loss=0.051, acc=96%\n",
      "1040) pretrained: loss=0.029, acc=98%\n",
      "1050) pretrained: loss=0.003, acc=100%\n",
      "1060) pretrained: loss=0.009, acc=100%\n",
      "1070) pretrained: loss=0.012, acc=100%\n",
      "1080) pretrained: loss=0.001, acc=100%\n",
      "1090) pretrained: loss=0.034, acc=98%\n",
      "1100) pretrained: loss=0.098, acc=98%\n",
      "1110) pretrained: loss=0.054, acc=98%\n",
      "1120) pretrained: loss=0.005, acc=100%\n",
      "1130) pretrained: loss=0.008, acc=100%\n",
      "1140) pretrained: loss=0.026, acc=100%\n",
      "1150) pretrained: loss=0.029, acc=100%\n",
      "1160) pretrained: loss=0.003, acc=100%\n",
      "1170) pretrained: loss=0.009, acc=100%\n",
      "1180) pretrained: loss=0.027, acc=100%\n",
      "1190) pretrained: loss=0.005, acc=100%\n",
      "1200) pretrained: loss=0.035, acc=98%\n",
      "1210) pretrained: loss=0.025, acc=98%\n",
      "1220) pretrained: loss=0.028, acc=100%\n",
      "1230) pretrained: loss=0.042, acc=98%\n",
      "1240) pretrained: loss=0.375, acc=94%\n",
      "1250) pretrained: loss=0.017, acc=100%\n",
      "1260) pretrained: loss=0.077, acc=96%\n",
      "1270) pretrained: loss=0.076, acc=96%\n",
      "1280) pretrained: loss=0.021, acc=100%\n",
      "1290) pretrained: loss=0.012, acc=100%\n",
      "1300) pretrained: loss=0.003, acc=100%\n",
      "1310) pretrained: loss=0.018, acc=100%\n",
      "1320) pretrained: loss=0.008, acc=100%\n",
      "1330) pretrained: loss=0.026, acc=98%\n",
      "1340) pretrained: loss=0.008, acc=100%\n",
      "1350) pretrained: loss=0.013, acc=100%\n",
      "1360) pretrained: loss=0.025, acc=100%\n",
      "1370) pretrained: loss=0.021, acc=100%\n",
      "1380) pretrained: loss=0.072, acc=96%\n",
      "1390) pretrained: loss=0.003, acc=100%\n",
      "1400) pretrained: loss=0.015, acc=100%\n",
      "1410) pretrained: loss=0.004, acc=100%\n",
      "1420) pretrained: loss=0.040, acc=98%\n",
      "1430) pretrained: loss=0.002, acc=100%\n",
      "1440) pretrained: loss=0.025, acc=98%\n",
      "1450) pretrained: loss=0.070, acc=98%\n",
      "1460) pretrained: loss=0.028, acc=98%\n",
      "1470) pretrained: loss=0.016, acc=100%\n",
      "1480) pretrained: loss=0.012, acc=100%\n",
      "1490) pretrained: loss=0.078, acc=98%\n",
      "1500) pretrained: loss=0.005, acc=100%\n",
      "1510) pretrained: loss=0.003, acc=100%\n",
      "1520) pretrained: loss=0.085, acc=98%\n",
      "1530) pretrained: loss=0.168, acc=94%\n",
      "1540) pretrained: loss=0.046, acc=98%\n",
      "1550) pretrained: loss=0.045, acc=98%\n",
      "1560) pretrained: loss=0.006, acc=100%\n",
      "1570) pretrained: loss=0.002, acc=100%\n",
      "1580) pretrained: loss=0.008, acc=100%\n",
      "1590) pretrained: loss=0.013, acc=100%\n",
      "1600) pretrained: loss=0.011, acc=100%\n",
      "1610) pretrained: loss=0.009, acc=100%\n",
      "1620) pretrained: loss=0.001, acc=100%\n",
      "1630) pretrained: loss=0.079, acc=96%\n",
      "1640) pretrained: loss=0.008, acc=100%\n",
      "1650) pretrained: loss=0.030, acc=98%\n",
      "1660) pretrained: loss=0.013, acc=100%\n",
      "1670) pretrained: loss=0.015, acc=100%\n",
      "1680) pretrained: loss=0.002, acc=100%\n",
      "1690) pretrained: loss=0.017, acc=100%\n",
      "1700) pretrained: loss=0.145, acc=98%\n",
      "1710) pretrained: loss=0.004, acc=100%\n",
      "1720) pretrained: loss=0.145, acc=96%\n",
      "1730) pretrained: loss=0.082, acc=98%\n",
      "1740) pretrained: loss=0.017, acc=100%\n",
      "1750) pretrained: loss=0.007, acc=100%\n",
      "1760) pretrained: loss=0.030, acc=100%\n",
      "1770) pretrained: loss=0.017, acc=100%\n",
      "1780) pretrained: loss=0.008, acc=100%\n",
      "1790) pretrained: loss=0.001, acc=100%\n",
      "1800) pretrained: loss=0.141, acc=94%\n",
      "1810) pretrained: loss=0.021, acc=100%\n",
      "1820) pretrained: loss=0.019, acc=100%\n",
      "1830) pretrained: loss=0.011, acc=100%\n",
      "1840) pretrained: loss=0.002, acc=100%\n",
      "1850) pretrained: loss=0.038, acc=98%\n",
      "1860) pretrained: loss=0.009, acc=100%\n",
      "1870) pretrained: loss=0.002, acc=100%\n",
      "1880) pretrained: loss=0.031, acc=98%\n",
      "1890) pretrained: loss=0.031, acc=100%\n",
      "1900) pretrained: loss=0.163, acc=96%\n",
      "1910) pretrained: loss=0.049, acc=98%\n",
      "1920) pretrained: loss=0.012, acc=100%\n",
      "1930) pretrained: loss=0.011, acc=100%\n",
      "1940) pretrained: loss=0.012, acc=100%\n",
      "1950) pretrained: loss=0.002, acc=100%\n",
      "1960) pretrained: loss=0.014, acc=100%\n",
      "1970) pretrained: loss=0.001, acc=100%\n",
      "1980) pretrained: loss=0.009, acc=100%\n",
      "1990) pretrained: loss=0.024, acc=98%\n",
      "2000) pretrained: loss=0.001, acc=100%\n",
      "2010) pretrained: loss=0.112, acc=96%\n",
      "2020) pretrained: loss=0.012, acc=100%\n",
      "2030) pretrained: loss=0.010, acc=100%\n",
      "2040) pretrained: loss=0.058, acc=98%\n",
      "2050) pretrained: loss=0.002, acc=100%\n",
      "2060) pretrained: loss=0.007, acc=100%\n",
      "2070) pretrained: loss=0.004, acc=100%\n",
      "2080) pretrained: loss=0.017, acc=98%\n",
      "2090) pretrained: loss=0.000, acc=100%\n",
      "2100) pretrained: loss=0.093, acc=98%\n",
      "2110) pretrained: loss=0.030, acc=100%\n",
      "2120) pretrained: loss=0.019, acc=100%\n",
      "2130) pretrained: loss=0.010, acc=100%\n",
      "2140) pretrained: loss=0.002, acc=100%\n",
      "2150) pretrained: loss=0.013, acc=100%\n",
      "2160) pretrained: loss=0.000, acc=100%\n",
      "2170) pretrained: loss=0.003, acc=100%\n",
      "2180) pretrained: loss=0.081, acc=98%\n",
      "2190) pretrained: loss=0.033, acc=98%\n",
      "2200) pretrained: loss=0.001, acc=100%\n",
      "2210) pretrained: loss=0.014, acc=100%\n",
      "2220) pretrained: loss=0.001, acc=100%\n",
      "2230) pretrained: loss=0.056, acc=98%\n",
      "2240) pretrained: loss=0.012, acc=100%\n",
      "2250) pretrained: loss=0.072, acc=98%\n",
      "2260) pretrained: loss=0.099, acc=98%\n",
      "2270) pretrained: loss=0.020, acc=100%\n",
      "2280) pretrained: loss=0.004, acc=100%\n",
      "2290) pretrained: loss=0.005, acc=100%\n",
      "2300) pretrained: loss=0.102, acc=98%\n",
      "2310) pretrained: loss=0.019, acc=98%\n",
      "2320) pretrained: loss=0.005, acc=100%\n",
      "2330) pretrained: loss=0.025, acc=100%\n",
      "2340) pretrained: loss=0.003, acc=100%\n",
      "2350) pretrained: loss=0.069, acc=98%\n",
      "2360) pretrained: loss=0.001, acc=100%\n",
      "2370) pretrained: loss=0.039, acc=98%\n",
      "2380) pretrained: loss=0.007, acc=100%\n",
      "2390) pretrained: loss=0.005, acc=100%\n",
      "2400) pretrained: loss=0.023, acc=100%\n",
      "2410) pretrained: loss=0.015, acc=100%\n",
      "2420) pretrained: loss=0.006, acc=100%\n",
      "2430) pretrained: loss=0.002, acc=100%\n",
      "2440) pretrained: loss=0.002, acc=100%\n",
      "2450) pretrained: loss=0.007, acc=100%\n",
      "2460) pretrained: loss=0.003, acc=100%\n",
      "2470) pretrained: loss=0.225, acc=92%\n",
      "2480) pretrained: loss=0.004, acc=100%\n",
      "2490) pretrained: loss=0.003, acc=100%\n",
      "2500) pretrained: loss=0.151, acc=96%\n",
      "2510) pretrained: loss=0.002, acc=100%\n",
      "2520) pretrained: loss=0.002, acc=100%\n",
      "2530) pretrained: loss=0.003, acc=100%\n",
      "2540) pretrained: loss=0.032, acc=98%\n",
      "2550) pretrained: loss=0.030, acc=98%\n",
      "2560) pretrained: loss=0.040, acc=98%\n",
      "2570) pretrained: loss=0.005, acc=100%\n",
      "2580) pretrained: loss=0.003, acc=100%\n",
      "2590) pretrained: loss=0.011, acc=100%\n",
      "2600) pretrained: loss=0.041, acc=98%\n",
      "2610) pretrained: loss=0.007, acc=100%\n",
      "2620) pretrained: loss=0.020, acc=98%\n",
      "2630) pretrained: loss=0.004, acc=100%\n",
      "2640) pretrained: loss=0.008, acc=100%\n",
      "2650) pretrained: loss=0.002, acc=100%\n",
      "2660) pretrained: loss=0.019, acc=98%\n",
      "2670) pretrained: loss=0.003, acc=100%\n",
      "2680) pretrained: loss=0.005, acc=100%\n",
      "2690) pretrained: loss=0.001, acc=100%\n",
      "2700) pretrained: loss=0.048, acc=98%\n",
      "2710) pretrained: loss=0.005, acc=100%\n",
      "2720) pretrained: loss=0.040, acc=98%\n",
      "2730) pretrained: loss=0.001, acc=100%\n",
      "2740) pretrained: loss=0.001, acc=100%\n",
      "2750) pretrained: loss=0.005, acc=100%\n",
      "2760) pretrained: loss=0.003, acc=100%\n",
      "2770) pretrained: loss=0.060, acc=98%\n",
      "2780) pretrained: loss=0.004, acc=100%\n",
      "2790) pretrained: loss=0.013, acc=100%\n",
      "2800) pretrained: loss=0.008, acc=100%\n",
      "2810) pretrained: loss=0.001, acc=100%\n",
      "2820) pretrained: loss=0.083, acc=96%\n",
      "2830) pretrained: loss=0.004, acc=100%\n",
      "2840) pretrained: loss=0.002, acc=100%\n",
      "2850) pretrained: loss=0.027, acc=98%\n",
      "2860) pretrained: loss=0.002, acc=100%\n",
      "2870) pretrained: loss=0.003, acc=100%\n",
      "2880) pretrained: loss=0.007, acc=100%\n",
      "2890) pretrained: loss=0.014, acc=100%\n",
      "2900) pretrained: loss=0.002, acc=100%\n",
      "2910) pretrained: loss=0.011, acc=100%\n",
      "2920) pretrained: loss=0.004, acc=100%\n",
      "2930) pretrained: loss=0.003, acc=100%\n",
      "2940) pretrained: loss=0.077, acc=96%\n",
      "2950) pretrained: loss=0.015, acc=100%\n",
      "2960) pretrained: loss=0.002, acc=100%\n",
      "2970) pretrained: loss=0.016, acc=100%\n",
      "2980) pretrained: loss=0.002, acc=100%\n",
      "2990) pretrained: loss=0.006, acc=100%\n",
      "3000) pretrained: loss=0.000, acc=100%\n",
      "3010) pretrained: loss=0.006, acc=100%\n",
      "3020) pretrained: loss=0.001, acc=100%\n",
      "3030) pretrained: loss=0.004, acc=100%\n",
      "3040) pretrained: loss=0.014, acc=100%\n",
      "3050) pretrained: loss=0.037, acc=98%\n",
      "3060) pretrained: loss=0.011, acc=100%\n",
      "3070) pretrained: loss=0.030, acc=98%\n",
      "3080) pretrained: loss=0.005, acc=100%\n",
      "3090) pretrained: loss=0.004, acc=100%\n",
      "3100) pretrained: loss=0.003, acc=100%\n",
      "3110) pretrained: loss=0.021, acc=98%\n",
      "3120) pretrained: loss=0.027, acc=98%\n",
      "3130) pretrained: loss=0.015, acc=100%\n",
      "3140) pretrained: loss=0.001, acc=100%\n",
      "3150) pretrained: loss=0.000, acc=100%\n",
      "3160) pretrained: loss=0.001, acc=100%\n",
      "3170) pretrained: loss=0.008, acc=100%\n",
      "3180) pretrained: loss=0.033, acc=98%\n",
      "3190) pretrained: loss=0.003, acc=100%\n",
      "3200) pretrained: loss=0.011, acc=100%\n",
      "3210) pretrained: loss=0.007, acc=100%\n",
      "3220) pretrained: loss=0.001, acc=100%\n",
      "3230) pretrained: loss=0.001, acc=100%\n",
      "3240) pretrained: loss=0.002, acc=100%\n",
      "3250) pretrained: loss=0.008, acc=100%\n",
      "3260) pretrained: loss=0.017, acc=98%\n",
      "3270) pretrained: loss=0.004, acc=100%\n",
      "3280) pretrained: loss=0.004, acc=100%\n",
      "3290) pretrained: loss=0.043, acc=98%\n",
      "3300) pretrained: loss=0.000, acc=100%\n",
      "3310) pretrained: loss=0.063, acc=96%\n",
      "3320) pretrained: loss=0.016, acc=100%\n",
      "3330) pretrained: loss=0.004, acc=100%\n",
      "3340) pretrained: loss=0.005, acc=100%\n",
      "3350) pretrained: loss=0.003, acc=100%\n",
      "3360) pretrained: loss=0.004, acc=100%\n",
      "3370) pretrained: loss=0.035, acc=98%\n",
      "3380) pretrained: loss=0.021, acc=98%\n",
      "3390) pretrained: loss=0.011, acc=100%\n",
      "3400) pretrained: loss=0.037, acc=96%\n",
      "3410) pretrained: loss=0.102, acc=98%\n",
      "3420) pretrained: loss=0.010, acc=100%\n",
      "3430) pretrained: loss=0.000, acc=100%\n",
      "3440) pretrained: loss=0.027, acc=98%\n",
      "3450) pretrained: loss=0.009, acc=100%\n",
      "3460) pretrained: loss=0.004, acc=100%\n",
      "3470) pretrained: loss=0.014, acc=100%\n",
      "3480) pretrained: loss=0.001, acc=100%\n",
      "3490) pretrained: loss=0.007, acc=100%\n",
      "3500) pretrained: loss=0.035, acc=98%\n",
      "3510) pretrained: loss=0.001, acc=100%\n",
      "3520) pretrained: loss=0.001, acc=100%\n",
      "3530) pretrained: loss=0.019, acc=100%\n",
      "3540) pretrained: loss=0.027, acc=98%\n",
      "3550) pretrained: loss=0.008, acc=100%\n",
      "3560) pretrained: loss=0.016, acc=100%\n",
      "3570) pretrained: loss=0.000, acc=100%\n",
      "3580) pretrained: loss=0.015, acc=100%\n",
      "3590) pretrained: loss=0.001, acc=100%\n",
      "3600) pretrained: loss=0.002, acc=100%\n",
      "3610) pretrained: loss=0.001, acc=100%\n",
      "3620) pretrained: loss=0.038, acc=98%\n",
      "3630) pretrained: loss=0.022, acc=98%\n",
      "3640) pretrained: loss=0.003, acc=100%\n",
      "3650) pretrained: loss=0.058, acc=98%\n",
      "3660) pretrained: loss=0.004, acc=100%\n",
      "3670) pretrained: loss=0.007, acc=100%\n",
      "3680) pretrained: loss=0.012, acc=100%\n",
      "3690) pretrained: loss=0.008, acc=100%\n",
      "3700) pretrained: loss=0.005, acc=100%\n",
      "3710) pretrained: loss=0.006, acc=100%\n",
      "3720) pretrained: loss=0.009, acc=100%\n",
      "3730) pretrained: loss=0.000, acc=100%\n",
      "3740) pretrained: loss=0.006, acc=100%\n",
      "3750) pretrained: loss=0.002, acc=100%\n",
      "3760) pretrained: loss=0.039, acc=98%\n",
      "3770) pretrained: loss=0.006, acc=100%\n",
      "3780) pretrained: loss=0.000, acc=100%\n",
      "3790) pretrained: loss=0.002, acc=100%\n",
      "3800) pretrained: loss=0.000, acc=100%\n",
      "3810) pretrained: loss=0.004, acc=100%\n",
      "3820) pretrained: loss=0.017, acc=100%\n",
      "3830) pretrained: loss=0.016, acc=100%\n",
      "3840) pretrained: loss=0.005, acc=100%\n",
      "3850) pretrained: loss=0.078, acc=96%\n",
      "3860) pretrained: loss=0.001, acc=100%\n",
      "3870) pretrained: loss=0.001, acc=100%\n",
      "3880) pretrained: loss=0.020, acc=98%\n",
      "3890) pretrained: loss=0.011, acc=100%\n",
      "3900) pretrained: loss=0.010, acc=100%\n",
      "3910) pretrained: loss=0.003, acc=100%\n",
      "3920) pretrained: loss=0.004, acc=100%\n",
      "3930) pretrained: loss=0.006, acc=100%\n",
      "3940) pretrained: loss=0.033, acc=98%\n",
      "3950) pretrained: loss=0.019, acc=98%\n",
      "3960) pretrained: loss=0.018, acc=98%\n",
      "3970) pretrained: loss=0.003, acc=100%\n",
      "3980) pretrained: loss=0.007, acc=100%\n",
      "3990) pretrained: loss=0.001, acc=100%\n",
      "4000) pretrained: loss=0.009, acc=100%\n",
      "4010) pretrained: loss=0.004, acc=100%\n",
      "4020) pretrained: loss=0.003, acc=100%\n",
      "4030) pretrained: loss=0.001, acc=100%\n",
      "4040) pretrained: loss=0.003, acc=100%\n",
      "4050) pretrained: loss=0.015, acc=100%\n",
      "4060) pretrained: loss=0.017, acc=98%\n",
      "4070) pretrained: loss=0.005, acc=100%\n",
      "4080) pretrained: loss=0.003, acc=100%\n",
      "4090) pretrained: loss=0.001, acc=100%\n",
      "4100) pretrained: loss=0.009, acc=100%\n",
      "4110) pretrained: loss=0.014, acc=100%\n",
      "4120) pretrained: loss=0.011, acc=100%\n",
      "4130) pretrained: loss=0.001, acc=100%\n",
      "4140) pretrained: loss=0.004, acc=100%\n",
      "4150) pretrained: loss=0.000, acc=100%\n",
      "4160) pretrained: loss=0.071, acc=98%\n",
      "4170) pretrained: loss=0.068, acc=98%\n",
      "4180) pretrained: loss=0.020, acc=100%\n",
      "4190) pretrained: loss=0.004, acc=100%\n",
      "4200) pretrained: loss=0.003, acc=100%\n",
      "4210) pretrained: loss=0.004, acc=100%\n",
      "4220) pretrained: loss=0.025, acc=98%\n",
      "4230) pretrained: loss=0.007, acc=100%\n",
      "4240) pretrained: loss=0.036, acc=98%\n",
      "4250) pretrained: loss=0.006, acc=100%\n",
      "4260) pretrained: loss=0.004, acc=100%\n",
      "4270) pretrained: loss=0.000, acc=100%\n",
      "4280) pretrained: loss=0.012, acc=100%\n",
      "4290) pretrained: loss=0.001, acc=100%\n",
      "4300) pretrained: loss=0.001, acc=100%\n",
      "4310) pretrained: loss=0.002, acc=100%\n",
      "4320) pretrained: loss=0.001, acc=100%\n",
      "4330) pretrained: loss=0.024, acc=100%\n",
      "4340) pretrained: loss=0.009, acc=100%\n",
      "4350) pretrained: loss=0.003, acc=100%\n",
      "4360) pretrained: loss=0.053, acc=96%\n",
      "4370) pretrained: loss=0.001, acc=100%\n",
      "4380) pretrained: loss=0.070, acc=98%\n",
      "4390) pretrained: loss=0.003, acc=100%\n",
      "4400) pretrained: loss=0.001, acc=100%\n",
      "4410) pretrained: loss=0.034, acc=98%\n",
      "4420) pretrained: loss=0.001, acc=100%\n",
      "4430) pretrained: loss=0.005, acc=100%\n",
      "4440) pretrained: loss=0.001, acc=100%\n",
      "4450) pretrained: loss=0.015, acc=100%\n",
      "4460) pretrained: loss=0.000, acc=100%\n",
      "4470) pretrained: loss=0.004, acc=100%\n",
      "4480) pretrained: loss=0.009, acc=100%\n",
      "4490) pretrained: loss=0.003, acc=100%\n",
      "4500) pretrained: loss=0.001, acc=100%\n",
      "4510) pretrained: loss=0.008, acc=100%\n",
      "4520) pretrained: loss=0.004, acc=100%\n",
      "4530) pretrained: loss=0.001, acc=100%\n",
      "4540) pretrained: loss=0.001, acc=100%\n",
      "4550) pretrained: loss=0.001, acc=100%\n",
      "4560) pretrained: loss=0.027, acc=98%\n",
      "4570) pretrained: loss=0.002, acc=100%\n",
      "4580) pretrained: loss=0.002, acc=100%\n",
      "4590) pretrained: loss=0.078, acc=98%\n",
      "4600) pretrained: loss=0.001, acc=100%\n",
      "4610) pretrained: loss=0.000, acc=100%\n",
      "4620) pretrained: loss=0.148, acc=96%\n",
      "4630) pretrained: loss=0.004, acc=100%\n",
      "4640) pretrained: loss=0.001, acc=100%\n",
      "4650) pretrained: loss=0.001, acc=100%\n",
      "4660) pretrained: loss=0.015, acc=100%\n",
      "4670) pretrained: loss=0.004, acc=100%\n",
      "4680) pretrained: loss=0.000, acc=100%\n",
      "4690) pretrained: loss=0.000, acc=100%\n",
      "4700) pretrained: loss=0.041, acc=98%\n",
      "4710) pretrained: loss=0.003, acc=100%\n",
      "4720) pretrained: loss=0.021, acc=98%\n",
      "4730) pretrained: loss=0.002, acc=100%\n",
      "4740) pretrained: loss=0.000, acc=100%\n",
      "4750) pretrained: loss=0.001, acc=100%\n",
      "4760) pretrained: loss=0.000, acc=100%\n",
      "4770) pretrained: loss=0.011, acc=100%\n",
      "4780) pretrained: loss=0.002, acc=100%\n",
      "4790) pretrained: loss=0.004, acc=100%\n",
      "4800) pretrained: loss=0.002, acc=100%\n",
      "4810) pretrained: loss=0.006, acc=100%\n",
      "4820) pretrained: loss=0.009, acc=100%\n",
      "4830) pretrained: loss=0.003, acc=100%\n",
      "4840) pretrained: loss=0.023, acc=100%\n",
      "4850) pretrained: loss=0.000, acc=100%\n",
      "4860) pretrained: loss=0.000, acc=100%\n",
      "4870) pretrained: loss=0.005, acc=100%\n",
      "4880) pretrained: loss=0.156, acc=94%\n",
      "4890) pretrained: loss=0.000, acc=100%\n",
      "4900) pretrained: loss=0.000, acc=100%\n",
      "4910) pretrained: loss=0.002, acc=100%\n",
      "4920) pretrained: loss=0.015, acc=98%\n",
      "4930) pretrained: loss=0.018, acc=98%\n",
      "4940) pretrained: loss=0.001, acc=100%\n",
      "4950) pretrained: loss=0.039, acc=98%\n",
      "4960) pretrained: loss=0.053, acc=98%\n",
      "4970) pretrained: loss=0.001, acc=100%\n",
      "4980) pretrained: loss=0.000, acc=100%\n",
      "4990) pretrained: loss=0.017, acc=98%\n",
      "5000) pretrained: loss=0.015, acc=98%\n",
      "5010) pretrained: loss=0.034, acc=98%\n",
      "5020) pretrained: loss=0.043, acc=98%\n",
      "5030) pretrained: loss=0.001, acc=100%\n",
      "5040) pretrained: loss=0.037, acc=98%\n",
      "5050) pretrained: loss=0.005, acc=100%\n",
      "5060) pretrained: loss=0.008, acc=100%\n",
      "5070) pretrained: loss=0.009, acc=100%\n",
      "5080) pretrained: loss=0.007, acc=100%\n",
      "5090) pretrained: loss=0.065, acc=98%\n",
      "5100) pretrained: loss=0.001, acc=100%\n",
      "5110) pretrained: loss=0.001, acc=100%\n",
      "5120) pretrained: loss=0.018, acc=100%\n",
      "5130) pretrained: loss=0.008, acc=100%\n",
      "5140) pretrained: loss=0.000, acc=100%\n",
      "5150) pretrained: loss=0.000, acc=100%\n",
      "5160) pretrained: loss=0.000, acc=100%\n",
      "5170) pretrained: loss=0.000, acc=100%\n",
      "5180) pretrained: loss=0.002, acc=100%\n",
      "5190) pretrained: loss=0.006, acc=100%\n",
      "5200) pretrained: loss=0.000, acc=100%\n",
      "5210) pretrained: loss=0.000, acc=100%\n",
      "5220) pretrained: loss=0.009, acc=100%\n",
      "5230) pretrained: loss=0.001, acc=100%\n",
      "5240) pretrained: loss=0.002, acc=100%\n",
      "5250) pretrained: loss=0.002, acc=100%\n",
      "5260) pretrained: loss=0.000, acc=100%\n",
      "5270) pretrained: loss=0.001, acc=100%\n",
      "5280) pretrained: loss=0.022, acc=98%\n",
      "5290) pretrained: loss=0.000, acc=100%\n",
      "5300) pretrained: loss=0.000, acc=100%\n",
      "5310) pretrained: loss=0.023, acc=98%\n",
      "5320) pretrained: loss=0.004, acc=100%\n",
      "5330) pretrained: loss=0.001, acc=100%\n",
      "5340) pretrained: loss=0.000, acc=100%\n",
      "5350) pretrained: loss=0.003, acc=100%\n",
      "5360) pretrained: loss=0.037, acc=98%\n",
      "5370) pretrained: loss=0.006, acc=100%\n",
      "5380) pretrained: loss=0.000, acc=100%\n",
      "5390) pretrained: loss=0.001, acc=100%\n",
      "5400) pretrained: loss=0.059, acc=98%\n",
      "5410) pretrained: loss=0.002, acc=100%\n",
      "5420) pretrained: loss=0.035, acc=98%\n",
      "5430) pretrained: loss=0.005, acc=100%\n",
      "5440) pretrained: loss=0.012, acc=100%\n",
      "5450) pretrained: loss=0.093, acc=98%\n",
      "5460) pretrained: loss=0.000, acc=100%\n",
      "5470) pretrained: loss=0.033, acc=98%\n",
      "5480) pretrained: loss=0.021, acc=98%\n",
      "5490) pretrained: loss=0.007, acc=100%\n",
      "5500) pretrained: loss=0.000, acc=100%\n",
      "5510) pretrained: loss=0.000, acc=100%\n",
      "5520) pretrained: loss=0.001, acc=100%\n",
      "5530) pretrained: loss=0.002, acc=100%\n",
      "5540) pretrained: loss=0.005, acc=100%\n",
      "5550) pretrained: loss=0.002, acc=100%\n",
      "5560) pretrained: loss=0.001, acc=100%\n",
      "5570) pretrained: loss=0.020, acc=98%\n",
      "5580) pretrained: loss=0.000, acc=100%\n",
      "5590) pretrained: loss=0.001, acc=100%\n",
      "5600) pretrained: loss=0.034, acc=98%\n",
      "5610) pretrained: loss=0.003, acc=100%\n",
      "5620) pretrained: loss=0.002, acc=100%\n",
      "5630) pretrained: loss=0.001, acc=100%\n",
      "5640) pretrained: loss=0.000, acc=100%\n",
      "5650) pretrained: loss=0.000, acc=100%\n",
      "5660) pretrained: loss=0.036, acc=98%\n",
      "5670) pretrained: loss=0.002, acc=100%\n",
      "5680) pretrained: loss=0.003, acc=100%\n",
      "5690) pretrained: loss=0.001, acc=100%\n",
      "5700) pretrained: loss=0.012, acc=100%\n",
      "5710) pretrained: loss=0.001, acc=100%\n",
      "5720) pretrained: loss=0.000, acc=100%\n",
      "5730) pretrained: loss=0.010, acc=100%\n",
      "5740) pretrained: loss=0.005, acc=100%\n",
      "5750) pretrained: loss=0.000, acc=100%\n",
      "5760) pretrained: loss=0.000, acc=100%\n",
      "5770) pretrained: loss=0.004, acc=100%\n",
      "5780) pretrained: loss=0.001, acc=100%\n",
      "5790) pretrained: loss=0.026, acc=98%\n",
      "5800) pretrained: loss=0.091, acc=98%\n",
      "5810) pretrained: loss=0.002, acc=100%\n",
      "5820) pretrained: loss=0.010, acc=100%\n",
      "5830) pretrained: loss=0.001, acc=100%\n",
      "5840) pretrained: loss=0.004, acc=100%\n",
      "5850) pretrained: loss=0.001, acc=100%\n",
      "5860) pretrained: loss=0.000, acc=100%\n",
      "5870) pretrained: loss=0.009, acc=100%\n",
      "5880) pretrained: loss=0.000, acc=100%\n",
      "5890) pretrained: loss=0.047, acc=96%\n",
      "5900) pretrained: loss=0.015, acc=98%\n",
      "5910) pretrained: loss=0.007, acc=100%\n",
      "5920) pretrained: loss=0.001, acc=100%\n",
      "5930) pretrained: loss=0.003, acc=100%\n",
      "5940) pretrained: loss=0.015, acc=100%\n",
      "5950) pretrained: loss=0.001, acc=100%\n",
      "5960) pretrained: loss=0.002, acc=100%\n",
      "5970) pretrained: loss=0.002, acc=100%\n",
      "5980) pretrained: loss=0.082, acc=96%\n",
      "5990) pretrained: loss=0.000, acc=100%\n",
      "6000) pretrained: loss=0.014, acc=100%\n",
      "6010) pretrained: loss=0.039, acc=98%\n",
      "6020) pretrained: loss=0.003, acc=100%\n",
      "6030) pretrained: loss=0.001, acc=100%\n",
      "6040) pretrained: loss=0.031, acc=98%\n",
      "6050) pretrained: loss=0.005, acc=100%\n",
      "6060) pretrained: loss=0.000, acc=100%\n",
      "6070) pretrained: loss=0.002, acc=100%\n",
      "6080) pretrained: loss=0.008, acc=100%\n",
      "6090) pretrained: loss=0.083, acc=98%\n",
      "6100) pretrained: loss=0.001, acc=100%\n",
      "6110) pretrained: loss=0.013, acc=100%\n",
      "6120) pretrained: loss=0.002, acc=100%\n",
      "6130) pretrained: loss=0.002, acc=100%\n",
      "6140) pretrained: loss=0.047, acc=98%\n",
      "6150) pretrained: loss=0.019, acc=98%\n",
      "6160) pretrained: loss=0.003, acc=100%\n",
      "6170) pretrained: loss=0.006, acc=100%\n",
      "6180) pretrained: loss=0.002, acc=100%\n",
      "6190) pretrained: loss=0.045, acc=98%\n",
      "6200) pretrained: loss=0.000, acc=100%\n",
      "6210) pretrained: loss=0.003, acc=100%\n",
      "6220) pretrained: loss=0.012, acc=100%\n",
      "6230) pretrained: loss=0.002, acc=100%\n",
      "6240) pretrained: loss=0.001, acc=100%\n",
      "6250) pretrained: loss=0.008, acc=100%\n",
      "6260) pretrained: loss=0.002, acc=100%\n",
      "6270) pretrained: loss=0.002, acc=100%\n",
      "6280) pretrained: loss=0.014, acc=98%\n",
      "6290) pretrained: loss=0.021, acc=98%\n",
      "6300) pretrained: loss=0.006, acc=100%\n",
      "6310) pretrained: loss=0.004, acc=100%\n",
      "6320) pretrained: loss=0.005, acc=100%\n",
      "6330) pretrained: loss=0.002, acc=100%\n",
      "6340) pretrained: loss=0.003, acc=100%\n",
      "6350) pretrained: loss=0.000, acc=100%\n",
      "6360) pretrained: loss=0.057, acc=96%\n",
      "6370) pretrained: loss=0.001, acc=100%\n",
      "6380) pretrained: loss=0.004, acc=100%\n",
      "6390) pretrained: loss=0.012, acc=100%\n",
      "6400) pretrained: loss=0.002, acc=100%\n",
      "6410) pretrained: loss=0.000, acc=100%\n",
      "6420) pretrained: loss=0.003, acc=100%\n",
      "6430) pretrained: loss=0.000, acc=100%\n",
      "6440) pretrained: loss=0.001, acc=100%\n",
      "6450) pretrained: loss=0.001, acc=100%\n",
      "6460) pretrained: loss=0.024, acc=98%\n",
      "6470) pretrained: loss=0.018, acc=100%\n",
      "6480) pretrained: loss=0.005, acc=100%\n",
      "6490) pretrained: loss=0.001, acc=100%\n",
      "6500) pretrained: loss=0.007, acc=100%\n",
      "6510) pretrained: loss=0.006, acc=100%\n",
      "6520) pretrained: loss=0.074, acc=98%\n",
      "6530) pretrained: loss=0.000, acc=100%\n",
      "6540) pretrained: loss=0.016, acc=100%\n",
      "6550) pretrained: loss=0.001, acc=100%\n",
      "6560) pretrained: loss=0.001, acc=100%\n",
      "6570) pretrained: loss=0.009, acc=100%\n",
      "6580) pretrained: loss=0.004, acc=100%\n",
      "6590) pretrained: loss=0.000, acc=100%\n",
      "6600) pretrained: loss=0.007, acc=100%\n",
      "6610) pretrained: loss=0.001, acc=100%\n",
      "6620) pretrained: loss=0.021, acc=98%\n",
      "6630) pretrained: loss=0.000, acc=100%\n",
      "6640) pretrained: loss=0.007, acc=100%\n",
      "6650) pretrained: loss=0.000, acc=100%\n",
      "6660) pretrained: loss=0.002, acc=100%\n",
      "6670) pretrained: loss=0.001, acc=100%\n",
      "6680) pretrained: loss=0.005, acc=100%\n",
      "6690) pretrained: loss=0.004, acc=100%\n",
      "6700) pretrained: loss=0.001, acc=100%\n",
      "6710) pretrained: loss=0.010, acc=100%\n",
      "6720) pretrained: loss=0.000, acc=100%\n",
      "6730) pretrained: loss=0.000, acc=100%\n",
      "6740) pretrained: loss=0.010, acc=100%\n",
      "6750) pretrained: loss=0.001, acc=100%\n",
      "6760) pretrained: loss=0.013, acc=100%\n",
      "6770) pretrained: loss=0.021, acc=98%\n",
      "6780) pretrained: loss=0.000, acc=100%\n",
      "6790) pretrained: loss=0.001, acc=100%\n",
      "6800) pretrained: loss=0.000, acc=100%\n",
      "6810) pretrained: loss=0.022, acc=100%\n",
      "6820) pretrained: loss=0.000, acc=100%\n",
      "6830) pretrained: loss=0.000, acc=100%\n",
      "6840) pretrained: loss=0.005, acc=100%\n",
      "6850) pretrained: loss=0.000, acc=100%\n",
      "6860) pretrained: loss=0.075, acc=96%\n",
      "6870) pretrained: loss=0.001, acc=100%\n",
      "6880) pretrained: loss=0.000, acc=100%\n",
      "6890) pretrained: loss=0.148, acc=94%\n",
      "6900) pretrained: loss=0.001, acc=100%\n",
      "6910) pretrained: loss=0.029, acc=98%\n",
      "6920) pretrained: loss=0.003, acc=100%\n",
      "6930) pretrained: loss=0.010, acc=100%\n",
      "6940) pretrained: loss=0.003, acc=100%\n",
      "6950) pretrained: loss=0.001, acc=100%\n",
      "6960) pretrained: loss=0.007, acc=100%\n",
      "6970) pretrained: loss=0.009, acc=100%\n",
      "6980) pretrained: loss=0.000, acc=100%\n",
      "6990) pretrained: loss=0.001, acc=100%\n",
      "7000) pretrained: loss=0.001, acc=100%\n",
      "7010) pretrained: loss=0.005, acc=100%\n",
      "7020) pretrained: loss=0.000, acc=100%\n",
      "7030) pretrained: loss=0.055, acc=96%\n",
      "7040) pretrained: loss=0.000, acc=100%\n",
      "7050) pretrained: loss=0.000, acc=100%\n",
      "7060) pretrained: loss=0.069, acc=98%\n",
      "7070) pretrained: loss=0.002, acc=100%\n",
      "7080) pretrained: loss=0.008, acc=100%\n",
      "7090) pretrained: loss=0.001, acc=100%\n",
      "7100) pretrained: loss=0.023, acc=98%\n",
      "7110) pretrained: loss=0.000, acc=100%\n",
      "7120) pretrained: loss=0.001, acc=100%\n",
      "7130) pretrained: loss=0.001, acc=100%\n",
      "7140) pretrained: loss=0.000, acc=100%\n",
      "7150) pretrained: loss=0.020, acc=98%\n",
      "7160) pretrained: loss=0.009, acc=100%\n",
      "7170) pretrained: loss=0.001, acc=100%\n",
      "7180) pretrained: loss=0.002, acc=100%\n",
      "7190) pretrained: loss=0.002, acc=100%\n",
      "7200) pretrained: loss=0.000, acc=100%\n",
      "7210) pretrained: loss=0.000, acc=100%\n",
      "7220) pretrained: loss=0.034, acc=98%\n",
      "7230) pretrained: loss=0.002, acc=100%\n",
      "7240) pretrained: loss=0.001, acc=100%\n",
      "7250) pretrained: loss=0.001, acc=100%\n",
      "7260) pretrained: loss=0.078, acc=98%\n",
      "7270) pretrained: loss=0.001, acc=100%\n",
      "7280) pretrained: loss=0.081, acc=98%\n",
      "7290) pretrained: loss=0.000, acc=100%\n",
      "7300) pretrained: loss=0.000, acc=100%\n",
      "7310) pretrained: loss=0.001, acc=100%\n",
      "7320) pretrained: loss=0.001, acc=100%\n",
      "7330) pretrained: loss=0.005, acc=100%\n",
      "7340) pretrained: loss=0.001, acc=100%\n",
      "7350) pretrained: loss=0.001, acc=100%\n",
      "7360) pretrained: loss=0.000, acc=100%\n",
      "7370) pretrained: loss=0.001, acc=100%\n",
      "7380) pretrained: loss=0.000, acc=100%\n",
      "7390) pretrained: loss=0.004, acc=100%\n",
      "7400) pretrained: loss=0.000, acc=100%\n",
      "7410) pretrained: loss=0.001, acc=100%\n",
      "7420) pretrained: loss=0.039, acc=98%\n",
      "7430) pretrained: loss=0.003, acc=100%\n",
      "7440) pretrained: loss=0.000, acc=100%\n",
      "7450) pretrained: loss=0.000, acc=100%\n",
      "7460) pretrained: loss=0.001, acc=100%\n",
      "7470) pretrained: loss=0.004, acc=100%\n",
      "7480) pretrained: loss=0.000, acc=100%\n",
      "7490) pretrained: loss=0.002, acc=100%\n",
      "7500) pretrained: loss=0.096, acc=96%\n",
      "7510) pretrained: loss=0.007, acc=100%\n",
      "7520) pretrained: loss=0.003, acc=100%\n",
      "7530) pretrained: loss=0.001, acc=100%\n",
      "7540) pretrained: loss=0.000, acc=100%\n",
      "7550) pretrained: loss=0.004, acc=100%\n",
      "7560) pretrained: loss=0.000, acc=100%\n",
      "7570) pretrained: loss=0.000, acc=100%\n",
      "7580) pretrained: loss=0.009, acc=100%\n",
      "7590) pretrained: loss=0.000, acc=100%\n",
      "7600) pretrained: loss=0.014, acc=100%\n",
      "7610) pretrained: loss=0.026, acc=98%\n",
      "7620) pretrained: loss=0.000, acc=100%\n",
      "7630) pretrained: loss=0.000, acc=100%\n",
      "7640) pretrained: loss=0.000, acc=100%\n",
      "7650) pretrained: loss=0.020, acc=98%\n",
      "7660) pretrained: loss=0.001, acc=100%\n",
      "7670) pretrained: loss=0.000, acc=100%\n",
      "7680) pretrained: loss=0.012, acc=100%\n",
      "7690) pretrained: loss=0.001, acc=100%\n",
      "7700) pretrained: loss=0.001, acc=100%\n",
      "7710) pretrained: loss=0.000, acc=100%\n",
      "7720) pretrained: loss=0.001, acc=100%\n",
      "7730) pretrained: loss=0.001, acc=100%\n",
      "7740) pretrained: loss=0.001, acc=100%\n",
      "7750) pretrained: loss=0.001, acc=100%\n",
      "7760) pretrained: loss=0.001, acc=100%\n",
      "7770) pretrained: loss=0.006, acc=100%\n",
      "7780) pretrained: loss=0.010, acc=100%\n",
      "7790) pretrained: loss=0.001, acc=100%\n",
      "7800) pretrained: loss=0.001, acc=100%\n",
      "7810) pretrained: loss=0.004, acc=100%\n",
      "7820) pretrained: loss=0.000, acc=100%\n",
      "7830) pretrained: loss=0.008, acc=100%\n",
      "7840) pretrained: loss=0.015, acc=98%\n",
      "7850) pretrained: loss=0.083, acc=96%\n",
      "7860) pretrained: loss=0.001, acc=100%\n",
      "7870) pretrained: loss=0.011, acc=100%\n",
      "7880) pretrained: loss=0.000, acc=100%\n",
      "7890) pretrained: loss=0.005, acc=100%\n",
      "7900) pretrained: loss=0.003, acc=100%\n",
      "7910) pretrained: loss=0.001, acc=100%\n",
      "7920) pretrained: loss=0.000, acc=100%\n",
      "7930) pretrained: loss=0.002, acc=100%\n",
      "7940) pretrained: loss=0.000, acc=100%\n",
      "7950) pretrained: loss=0.001, acc=100%\n",
      "7960) pretrained: loss=0.005, acc=100%\n",
      "7970) pretrained: loss=0.024, acc=98%\n",
      "7980) pretrained: loss=0.002, acc=100%\n",
      "7990) pretrained: loss=0.000, acc=100%\n",
      "8000) pretrained: loss=0.021, acc=98%\n",
      "8010) pretrained: loss=0.002, acc=100%\n",
      "8020) pretrained: loss=0.001, acc=100%\n",
      "8030) pretrained: loss=0.001, acc=100%\n",
      "8040) pretrained: loss=0.000, acc=100%\n",
      "8050) pretrained: loss=0.002, acc=100%\n",
      "8060) pretrained: loss=0.044, acc=98%\n",
      "8070) pretrained: loss=0.000, acc=100%\n",
      "8080) pretrained: loss=0.000, acc=100%\n",
      "8090) pretrained: loss=0.000, acc=100%\n",
      "8100) pretrained: loss=0.011, acc=100%\n",
      "8110) pretrained: loss=0.000, acc=100%\n",
      "8120) pretrained: loss=0.036, acc=98%\n",
      "8130) pretrained: loss=0.003, acc=100%\n",
      "8140) pretrained: loss=0.008, acc=100%\n",
      "8150) pretrained: loss=0.001, acc=100%\n",
      "8160) pretrained: loss=0.001, acc=100%\n",
      "8170) pretrained: loss=0.000, acc=100%\n",
      "8180) pretrained: loss=0.008, acc=100%\n",
      "8190) pretrained: loss=0.000, acc=100%\n",
      "8200) pretrained: loss=0.001, acc=100%\n",
      "8210) pretrained: loss=0.042, acc=98%\n",
      "8220) pretrained: loss=0.002, acc=100%\n",
      "8230) pretrained: loss=0.000, acc=100%\n",
      "8240) pretrained: loss=0.003, acc=100%\n",
      "8250) pretrained: loss=0.000, acc=100%\n",
      "8260) pretrained: loss=0.003, acc=100%\n",
      "8270) pretrained: loss=0.000, acc=100%\n",
      "8280) pretrained: loss=0.001, acc=100%\n",
      "8290) pretrained: loss=0.000, acc=100%\n",
      "8300) pretrained: loss=0.057, acc=98%\n",
      "8310) pretrained: loss=0.000, acc=100%\n",
      "8320) pretrained: loss=0.001, acc=100%\n",
      "8330) pretrained: loss=0.009, acc=100%\n",
      "8340) pretrained: loss=0.000, acc=100%\n",
      "8350) pretrained: loss=0.000, acc=100%\n",
      "8360) pretrained: loss=0.000, acc=100%\n",
      "8370) pretrained: loss=0.003, acc=100%\n",
      "8380) pretrained: loss=0.009, acc=100%\n",
      "8390) pretrained: loss=0.056, acc=98%\n",
      "8400) pretrained: loss=0.000, acc=100%\n",
      "8410) pretrained: loss=0.000, acc=100%\n",
      "8420) pretrained: loss=0.007, acc=100%\n",
      "8430) pretrained: loss=0.000, acc=100%\n",
      "8440) pretrained: loss=0.007, acc=100%\n",
      "8450) pretrained: loss=0.000, acc=100%\n",
      "8460) pretrained: loss=0.000, acc=100%\n",
      "8470) pretrained: loss=0.003, acc=100%\n",
      "8480) pretrained: loss=0.001, acc=100%\n",
      "8490) pretrained: loss=0.002, acc=100%\n",
      "8500) pretrained: loss=0.017, acc=98%\n",
      "8510) pretrained: loss=0.005, acc=100%\n",
      "8520) pretrained: loss=0.000, acc=100%\n",
      "8530) pretrained: loss=0.003, acc=100%\n",
      "8540) pretrained: loss=0.000, acc=100%\n",
      "8550) pretrained: loss=0.000, acc=100%\n",
      "8560) pretrained: loss=0.000, acc=100%\n",
      "8570) pretrained: loss=0.000, acc=100%\n",
      "8580) pretrained: loss=0.003, acc=100%\n",
      "8590) pretrained: loss=0.000, acc=100%\n",
      "8600) pretrained: loss=0.001, acc=100%\n",
      "8610) pretrained: loss=0.000, acc=100%\n",
      "8620) pretrained: loss=0.005, acc=100%\n",
      "8630) pretrained: loss=0.005, acc=100%\n",
      "8640) pretrained: loss=0.000, acc=100%\n",
      "8650) pretrained: loss=0.000, acc=100%\n",
      "8660) pretrained: loss=0.003, acc=100%\n",
      "8670) pretrained: loss=0.000, acc=100%\n",
      "8680) pretrained: loss=0.005, acc=100%\n",
      "8690) pretrained: loss=0.010, acc=100%\n",
      "8700) pretrained: loss=0.000, acc=100%\n",
      "8710) pretrained: loss=0.002, acc=100%\n",
      "8720) pretrained: loss=0.002, acc=100%\n",
      "8730) pretrained: loss=0.001, acc=100%\n",
      "8740) pretrained: loss=0.000, acc=100%\n",
      "8750) pretrained: loss=0.000, acc=100%\n",
      "8760) pretrained: loss=0.001, acc=100%\n",
      "8770) pretrained: loss=0.012, acc=100%\n",
      "8780) pretrained: loss=0.000, acc=100%\n",
      "8790) pretrained: loss=0.001, acc=100%\n",
      "8800) pretrained: loss=0.000, acc=100%\n",
      "8810) pretrained: loss=0.000, acc=100%\n",
      "8820) pretrained: loss=0.019, acc=98%\n",
      "8830) pretrained: loss=0.001, acc=100%\n",
      "8840) pretrained: loss=0.000, acc=100%\n",
      "8850) pretrained: loss=0.000, acc=100%\n",
      "8860) pretrained: loss=0.001, acc=100%\n",
      "8870) pretrained: loss=0.000, acc=100%\n",
      "8880) pretrained: loss=0.001, acc=100%\n",
      "8890) pretrained: loss=0.017, acc=100%\n",
      "8900) pretrained: loss=0.001, acc=100%\n",
      "8910) pretrained: loss=0.004, acc=100%\n",
      "8920) pretrained: loss=0.003, acc=100%\n",
      "8930) pretrained: loss=0.003, acc=100%\n",
      "8940) pretrained: loss=0.000, acc=100%\n",
      "8950) pretrained: loss=0.000, acc=100%\n",
      "8960) pretrained: loss=0.004, acc=100%\n",
      "8970) pretrained: loss=0.000, acc=100%\n",
      "8980) pretrained: loss=0.000, acc=100%\n",
      "8990) pretrained: loss=0.005, acc=100%\n",
      "9000) pretrained: loss=0.003, acc=100%\n",
      "9010) pretrained: loss=0.014, acc=100%\n",
      "9020) pretrained: loss=0.000, acc=100%\n",
      "9030) pretrained: loss=0.017, acc=98%\n",
      "9040) pretrained: loss=0.000, acc=100%\n",
      "9050) pretrained: loss=0.000, acc=100%\n",
      "9060) pretrained: loss=0.035, acc=98%\n",
      "9070) pretrained: loss=0.007, acc=100%\n",
      "9080) pretrained: loss=0.002, acc=100%\n",
      "9090) pretrained: loss=0.001, acc=100%\n",
      "9100) pretrained: loss=0.001, acc=100%\n",
      "9110) pretrained: loss=0.000, acc=100%\n",
      "9120) pretrained: loss=0.001, acc=100%\n",
      "9130) pretrained: loss=0.013, acc=100%\n",
      "9140) pretrained: loss=0.003, acc=100%\n",
      "9150) pretrained: loss=0.004, acc=100%\n",
      "9160) pretrained: loss=0.000, acc=100%\n",
      "9170) pretrained: loss=0.000, acc=100%\n",
      "9180) pretrained: loss=0.003, acc=100%\n",
      "9190) pretrained: loss=0.000, acc=100%\n",
      "9200) pretrained: loss=0.004, acc=100%\n",
      "9210) pretrained: loss=0.064, acc=98%\n",
      "9220) pretrained: loss=0.002, acc=100%\n",
      "9230) pretrained: loss=0.001, acc=100%\n",
      "9240) pretrained: loss=0.000, acc=100%\n",
      "9250) pretrained: loss=0.000, acc=100%\n",
      "9260) pretrained: loss=0.008, acc=100%\n",
      "9270) pretrained: loss=0.003, acc=100%\n",
      "9280) pretrained: loss=0.006, acc=100%\n",
      "9290) pretrained: loss=0.000, acc=100%\n",
      "9300) pretrained: loss=0.001, acc=100%\n",
      "9310) pretrained: loss=0.000, acc=100%\n",
      "9320) pretrained: loss=0.000, acc=100%\n",
      "9330) pretrained: loss=0.001, acc=100%\n",
      "9340) pretrained: loss=0.000, acc=100%\n",
      "9350) pretrained: loss=0.001, acc=100%\n",
      "9360) pretrained: loss=0.002, acc=100%\n",
      "9370) pretrained: loss=0.001, acc=100%\n",
      "9380) pretrained: loss=0.070, acc=98%\n",
      "9390) pretrained: loss=0.001, acc=100%\n",
      "9400) pretrained: loss=0.001, acc=100%\n",
      "9410) pretrained: loss=0.000, acc=100%\n",
      "9420) pretrained: loss=0.000, acc=100%\n",
      "9430) pretrained: loss=0.000, acc=100%\n",
      "9440) pretrained: loss=0.069, acc=98%\n",
      "9450) pretrained: loss=0.000, acc=100%\n",
      "9460) pretrained: loss=0.001, acc=100%\n",
      "9470) pretrained: loss=0.000, acc=100%\n",
      "9480) pretrained: loss=0.037, acc=98%\n",
      "9490) pretrained: loss=0.016, acc=98%\n",
      "9500) pretrained: loss=0.000, acc=100%\n",
      "9510) pretrained: loss=0.037, acc=98%\n",
      "9520) pretrained: loss=0.015, acc=98%\n",
      "9530) pretrained: loss=0.000, acc=100%\n",
      "9540) pretrained: loss=0.000, acc=100%\n",
      "9550) pretrained: loss=0.003, acc=100%\n",
      "9560) pretrained: loss=0.001, acc=100%\n",
      "9570) pretrained: loss=0.001, acc=100%\n",
      "9580) pretrained: loss=0.000, acc=100%\n",
      "9590) pretrained: loss=0.002, acc=100%\n",
      "9600) pretrained: loss=0.001, acc=100%\n",
      "9610) pretrained: loss=0.002, acc=100%\n",
      "9620) pretrained: loss=0.045, acc=98%\n",
      "9630) pretrained: loss=0.000, acc=100%\n",
      "9640) pretrained: loss=0.003, acc=100%\n",
      "9650) pretrained: loss=0.003, acc=100%\n",
      "9660) pretrained: loss=0.000, acc=100%\n",
      "9670) pretrained: loss=0.001, acc=100%\n",
      "9680) pretrained: loss=0.001, acc=100%\n",
      "9690) pretrained: loss=0.000, acc=100%\n",
      "9700) pretrained: loss=0.000, acc=100%\n",
      "9710) pretrained: loss=0.000, acc=100%\n",
      "9720) pretrained: loss=0.000, acc=100%\n",
      "9730) pretrained: loss=0.000, acc=100%\n",
      "9740) pretrained: loss=0.001, acc=100%\n",
      "9750) pretrained: loss=0.003, acc=100%\n",
      "9760) pretrained: loss=0.000, acc=100%\n",
      "9770) pretrained: loss=0.001, acc=100%\n",
      "9780) pretrained: loss=0.000, acc=100%\n",
      "9790) pretrained: loss=0.000, acc=100%\n",
      "9800) pretrained: loss=0.003, acc=100%\n",
      "9810) pretrained: loss=0.003, acc=100%\n",
      "9820) pretrained: loss=0.000, acc=100%\n",
      "9830) pretrained: loss=0.000, acc=100%\n",
      "9840) pretrained: loss=0.000, acc=100%\n",
      "9850) pretrained: loss=0.001, acc=100%\n",
      "9860) pretrained: loss=0.001, acc=100%\n",
      "9870) pretrained: loss=0.037, acc=96%\n",
      "9880) pretrained: loss=0.001, acc=100%\n",
      "9890) pretrained: loss=0.000, acc=100%\n",
      "9900) pretrained: loss=0.000, acc=100%\n",
      "9910) pretrained: loss=0.000, acc=100%\n",
      "9920) pretrained: loss=0.000, acc=100%\n",
      "9930) pretrained: loss=0.000, acc=100%\n",
      "9940) pretrained: loss=0.001, acc=100%\n",
      "9950) pretrained: loss=0.011, acc=100%\n",
      "9960) pretrained: loss=0.000, acc=100%\n",
      "9970) pretrained: loss=0.000, acc=100%\n",
      "9980) pretrained: loss=0.000, acc=100%\n",
      "9990) pretrained: loss=0.000, acc=100%\n",
      "9999) pretrained: loss=0.000, acc=100%\n",
      "Done.\n",
      "trained weights /tmp/tmppDlBSX/weights.pretrained.caffemodel\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEPCAYAAABhkeIdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d825b4d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "niter = 10000\n",
    "\n",
    "# Reset style_solver as before.\n",
    "style_solver_filename = solver(create_style_recognize_net(train = True, learn_all = True))\n",
    "print(style_solver_filename)\n",
    "style_solver = caffe.get_solver(style_solver_filename)\n",
    "style_solver.net.copy_from(pretrained_weights_path)\n",
    "\n",
    "print 'Running solver for %d iterations...' % niter\n",
    "solvers = [('pretrained', style_solver)]\n",
    "loss, acc, weights = run_solvers(niter, solvers)\n",
    "print 'Done.'\n",
    "\n",
    "train_loss = loss['pretrained']\n",
    "train_acc = acc['pretrained']\n",
    "style_weights = weights['pretrained']\n",
    "print 'trained weights', style_weights\n",
    "print(train_loss.shape)\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(train_loss[200:].T)\n",
    "ax[0].set_xlabel('Iteration #')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(train_acc[200:].T)\n",
    "ax[1].set_xlabel('Iteration #')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if first 1525 got 3 diff and 1522 same\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "\n",
    "def infer_styles(model, infer_weigths, outfile):\n",
    "    #mean_image_npy = caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy'\n",
    "    mean_image_npy = caffe_root + 'data/ilsvrc12/scene_ass_mean.npy'\n",
    "    assert os.path.isfile(mean_image_npy), mean_image_npy\n",
    "    test_net = caffe.Classifier(model,\n",
    "                                infer_weigths,\n",
    "                                mean = \\\n",
    "                                    np.load(mean_image_npy).mean(1).mean(1),\n",
    "                                channel_swap=(2,1,0),\n",
    "                                raw_scale=255,\n",
    "                                image_dims=(227, 227))\n",
    "\n",
    "    with open(outfile, 'w') as the_file:        \n",
    "        writer = csv.writer(the_file, delimiter = ',')\n",
    "        field_names = ['Id', 'Category']\n",
    "        writer.writerow(field_names)\n",
    "        set_fullpath = '/home/dmitri/projects/graphicon_paper/dataset/video10sh_train_'\n",
    "        count_diff = 0\n",
    "        count_same = 0\n",
    "        for image_path in os.listdir(set_fullpath):\n",
    "            if not image_path.endswith('png'):\n",
    "                continue\n",
    "            full_image_path = os.path.join(set_fullpath, image_path)\n",
    "            \n",
    "            #caffe.io.load_image\n",
    "            predictions = test_net.predict([sp.ndimage.imread(full_image_path)])[0]\n",
    "            #print(predictions)\n",
    "            prediction = np.argmax(predictions)\n",
    "            if 0 == prediction:\n",
    "                #print('did image {}'.format(full_image_path))   \n",
    "                #print(predictions)\n",
    "                count_diff += 1\n",
    "            else:\n",
    "                count_same += 1\n",
    "            writer.writerow([image_path.split('.')[0], str(prediction)])\n",
    "        print('if first 1525 got {} diff and {} same'.format(count_diff, count_same))\n",
    "            \n",
    "\n",
    "#model = './basic_canvasstylenet'\n",
    "model = 'alex_deploy'#tmpYivBDh_deploy'#tmpJFKGLk_deploy'\n",
    "\n",
    "\n",
    "current_weigths = 'weights.pretrained.caffemodel2'\n",
    "\n",
    "outfile = 'scenes_infered.csv'\n",
    "infer_styles(model, current_weigths, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if first 1525 got 12 diff and 1513 same\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def get_image_distance(f1, f2):\n",
    "    ffs = []\n",
    "    for f in [f1, f2]:\n",
    "        ffs.append(f.astype(float) / 255)\n",
    "\n",
    "    diff = ffs[0] - ffs[1]\n",
    "    for ch, chn in enumerate(['r', 'g', 'b']):\n",
    "        diff_ch_range = np.max(diff[:, :, ch]) - np.min(diff[:, :, ch])\n",
    "        diff[:, :, ch] = diff[:, :, ch] / diff_ch_range\n",
    "        diff[:, :, ch] = diff[:, :, ch] - np.min(diff[:, :, ch])\n",
    "        diff *= 255.  \n",
    "        return diff.astype(np.uint8)\n",
    "\n",
    "def infer_styles(model, infer_weigths, outfile):\n",
    "    #mean_image_npy = caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy'\n",
    "    mean_image_npy = caffe_root + 'data/ilsvrc12/scene_ass_mean.npy'\n",
    "    assert os.path.isfile(mean_image_npy), mean_image_npy\n",
    "    test_net = caffe.Classifier(model,\n",
    "                                infer_weigths,\n",
    "                                mean = \\\n",
    "                                    np.load(mean_image_npy).mean(1).mean(1),\n",
    "                                channel_swap=(2,1,0),\n",
    "                                raw_scale=255,\n",
    "                                image_dims=(227, 227))\n",
    "\n",
    "    with open(outfile, 'w') as the_file:        \n",
    "        writer = csv.writer(the_file, delimiter = ',')\n",
    "        field_names = ['Id', 'Category']\n",
    "        writer.writerow(field_names)\n",
    "        set_fullpath = '/home/dmitri/projects/graphicon_paper/dataset/video10sh'\n",
    "        count_diff = 0\n",
    "        count_same = 0\n",
    "        all_ = 1525\n",
    "        for image_path_i in os.listdir(set_fullpath):\n",
    "            if not image_path_i.endswith('png'):\n",
    "                continue\n",
    "            full_image_path_i = os.path.join(set_fullpath, image_path_i)\n",
    "            for image_path_j in os.listdir(set_fullpath):\n",
    "                if not image_path_j.endswith('png'):\n",
    "                    continue\n",
    "                full_image_path_j = os.path.join(set_fullpath, image_path_j)\n",
    "                if full_image_path_j == full_image_path_i:\n",
    "                    continue\n",
    "\n",
    "                if count_diff + count_same >= all_:\n",
    "                    break\n",
    "                f1 = caffe.io.load_image(full_image_path_i)\n",
    "                f2 = caffe.io.load_image(full_image_path_j)\n",
    "                diff = get_image_distance(f1, f2)\n",
    "                predictions = test_net.predict([diff])[0]\n",
    "                #print(predictions)\n",
    "                prediction = np.argmax(predictions)\n",
    "                if 0 == prediction:\n",
    "                    #print('did image {}'.format(full_image_path))   \n",
    "                    #print(predictions)\n",
    "                    count_diff += 1\n",
    "                else:\n",
    "                    count_same += 1\n",
    "                writer.writerow([full_image_path_i.split('.')[0] + full_image_path_j.split('.')[0],  str(prediction)])\n",
    "                \n",
    "        print('if first 1525 got {} diff and {} same'.format(count_diff, count_same))\n",
    "            \n",
    "\n",
    "#model = './basic_canvasstylenet'\n",
    "model = 'alex_deploy'#tmpYivBDh_deploy'#tmpJFKGLk_deploy'\n",
    "\n",
    "\n",
    "current_weigths = 'weights.pretrained.caffemodel2'\n",
    "\n",
    "outfile = 'scenes_infered.csv'\n",
    "infer_styles(model, current_weigths, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
